==============================================
üî¨ LoRA Adapter Architecture Visualizer
==============================================
Job ID: 636815
Node: chimera21
GPUs: 0
Start time: Sun Oct 19 10:14:38 PM EDT 2025
==============================================

GPU Information:
NVIDIA H200, 143771 MiB


================================================================================
 LoRA ADAPTER ARCHITECTURE VISUALIZER
================================================================================

Loading base model...
‚úì Loaded

================================================================================
 BASE MODEL STRUCTURE
================================================================================

üìä Total Parameters: 4.30B (4,300,079,472)
üíæ Approximate Size: 8.01 GB (bfloat16)

üîí Status: All parameters are FROZEN during LoRA training
   ‚Üí Original capabilities preserved!

üìã Sample Layer Names:
   model.vision_tower.vision_model.embeddings.patch_embedding.weight: torch.Size([1152, 3, 14, 14])
   model.vision_tower.vision_model.embeddings.patch_embedding.bias: torch.Size([1152])
   model.vision_tower.vision_model.embeddings.position_embedding.weight: torch.Size([4096, 1152])
   model.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([1152])
   model.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([1152])
   model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([1152, 1152])
   model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([1152])
   model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([1152, 1152])
   model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([1152])
   model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([1152, 1152])
   ... (883 total layers)

================================================================================
 LOADING LORA ADAPTER
================================================================================

Adapter Path: /hpcstor6/scratch01/e/edward.gaibor001/medgemma_runs/omama256_balanced
Loading adapter on top of base model...

‚úì Adapter loaded

================================================================================
 ADAPTER STRUCTURE
================================================================================

üìä Total Parameters: 5.68B (5,681,082,224)
üéØ Trainable (LoRA): 1.34B (1,342,504,960)
üîí Frozen (Base):    4.34B

üí° Adapter Size: Only 31.220% of base model!
üíæ Adapter Storage: ~2560.6 MB

üìã Adapter Layers Added:

   Found 0 LoRA adapter layers:


================================================================================
 HOW LORA WORKS - THE MATH
================================================================================

Original Layer (Frozen):
   output = W √ó input
   where W is a large weight matrix (e.g., 4096 √ó 4096 = 16.7M parameters)

LoRA Adapter (Trainable):
   output = W √ó input + (B √ó A) √ó input
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          LoRA adapter
   where:
   ‚Ä¢ A is a small matrix: 4096 √ó r (r = rank, typically 8-64)
   ‚Ä¢ B is a small matrix: r √ó 4096
   ‚Ä¢ B √ó A approximates the weight update

Example with rank=16:
   Original W: 4096 √ó 4096 = 16,777,216 parameters
   LoRA (A+B): (4096√ó16) + (16√ó4096) = 131,072 parameters
   Reduction: 99.2% fewer parameters! üéâ

Why This Works:
   ‚Ä¢ Most weight updates are "low-rank" (can be approximated by smaller matrices)
   ‚Ä¢ We only train A and B, keeping W frozen
   ‚Ä¢ At inference: output = W √ó input + Œ± √ó (B √ó A) √ó input
     where Œ± is a scaling factor (lora_alpha / lora_rank)

================================================================================
 ADAPTER CONFIGURATION
================================================================================

Your LoRA config (from training):

   ‚Ä¢ Rank (r): 16
   ‚Ä¢ Alpha: 16
   ‚Ä¢ Dropout: 0.05
   ‚Ä¢ Target modules: all-linear
   ‚Ä¢ Task type: CAUSAL_LM
   ‚Ä¢ Modules saved: lm_head, embed_tokens


üí° What this means:
   ‚Ä¢ Rank 16 = good balance of capacity vs. efficiency
   ‚Ä¢ Alpha 16 = scaling factor for adapter contribution
   ‚Ä¢ All-linear = applies LoRA to all linear layers
   ‚Ä¢ lm_head/embed_tokens also fine-tuned for classification

================================================================================
 MEMORY EFFICIENCY COMPARISON
================================================================================

‚ùå Fine-tuning entire model:
   ‚Ä¢ All 4.30B parameters trainable
   ‚Ä¢ Optimizer states: 3√ó model size = 24.0 GB
   ‚Ä¢ Gradients: 1√ó model size = 8.0 GB
   ‚Ä¢ Total GPU memory: ~32.0 GB

‚úÖ LoRA fine-tuning:
   ‚Ä¢ Only 1.34B parameters trainable
   ‚Ä¢ Optimizer states: 3√ó adapter = 7.50 GB
   ‚Ä¢ Gradients: 1√ó adapter = 2.50 GB
   ‚Ä¢ Total GPU memory: ~18.0 GB
   ‚Ä¢ Savings: ~43.8%

================================================================================
 DEPLOYMENT SCENARIOS
================================================================================

üè• Scenario 1: Single Task
   ‚Ä¢ Base model: 8 GB
   ‚Ä¢ 1 Adapter: 10 MB
   ‚Ä¢ Total: ~8 GB

üè• Scenario 2: Multiple Tasks (Your Use Case!)
   ‚Ä¢ Base model: 8 GB
   ‚Ä¢ Classification adapter: 10 MB
   ‚Ä¢ Explanation adapter: 10 MB
   ‚Ä¢ Report generation adapter: 10 MB
   ‚Ä¢ Total: ~8.03 GB

   Compare to fine-tuning 3 separate models: 24 GB!
   üíæ Space savings: 66%

================================================================================
 SWITCHING BETWEEN MODES
================================================================================

# Load base model once
base = AutoModelForImageTextToText.from_pretrained("medgemma-4b-it")

# Mode 1: Classification
classifier = PeftModel.from_pretrained(base, "classification_adapter")
output = classifier.generate(...)  # "B: Cancer"
classifier.unload()  # Remove adapter

# Mode 2: Conversational (base model restored)
output = base.generate(...)  # Detailed explanation

# Mode 3: Load different adapter
explainer = PeftModel.from_pretrained(base, "explanation_adapter")
output = explainer.generate(...)  # Structured findings
explainer.unload()

‚ö° Fast switching: Only loading/unloading ~10MB adapter!

================================================================================
 SUMMARY
================================================================================

‚ú® Your LoRA Adapter Stats:
   ‚Ä¢ Base model size: 4.30B parameters
   ‚Ä¢ Adapter size: 1.34B parameters
   ‚Ä¢ Ratio: 31.220%
   ‚Ä¢ Training time: ~31.2% of full fine-tuning
   ‚Ä¢ Storage: Only 2560.6 MB per adapter

üéØ Key Benefits:
   1. Preserves base model capabilities (conversational)
   2. Adds specialized knowledge (classification)
   3. Memory efficient (99% smaller than full model)
   4. Fast training (only updates 31.2% of parameters)
   5. Modular (train multiple adapters for different tasks)

üí° Your Hierarchy:
   Base Model (Conversational) ‚Üê 4B params
        ‚Üì
   + Classification Adapter ‚Üê 1.34B params
        ‚Üì
   = Specialized Classifier that can still access base knowledge!

================================================================================


==============================================
‚úÖ Visualization Complete!
End time: Sun Oct 19 10:15:03 PM EDT 2025
==============================================
