==============================================
Step 3: Train with Balanced Dataset
==============================================
Job ID: 636196
Node: chimera21
GPUs: 0
Start time: Fri Oct 17 07:29:06 PM EDT 2025
==============================================

GPU Information:
NVIDIA H200, 143771 MiB

============================================================
ðŸ”¬ TRAINING WITH BALANCED DATASET (50/50)
============================================================
Data: /hpcstor6/scratch01/e/edward.gaibor001/omamadata256/hf_proc_messages_balanced
Output: /hpcstor6/scratch01/e/edward.gaibor001/medgemma_runs/omama256_balanced
============================================================

Loading google/medgemma-4b-it (attn=eager, 4bit=False)

============================================================
ðŸ“Š DATASET DISTRIBUTION
============================================================
Train set (11,760 samples):
  NonCancer: 5,880 (50.0%)
  Cancer: 5,880 (50.0%)

Eval set (1,000 samples):
  NonCancer: 486 (48.6%)
  Cancer: 514 (51.4%)
============================================================


============================================================
ðŸš€ STARTING TRAINING WITH BALANCED DATA
============================================================
Expected improvements:
  âœ… No class imbalance bias
  âœ… Model learns to detect both classes equally
  âœ… Should see 75-85% sensitivity (vs 16% before)
  âœ… Slightly lower overall accuracy (90-92% vs 96%)
  âœ… But much better clinical utility!
============================================================

{'loss': 5.5098, 'grad_norm': 0.43789157271385193, 'learning_rate': 0.00016540084388185655, 'entropy': 1.4134752830863, 'num_tokens': 692403.0, 'mean_token_accuracy': 0.8434825412929058, 'epoch': 0.2}
{'loss': 0.2876, 'grad_norm': 3.1775784492492676, 'learning_rate': 0.00012320675105485234, 'entropy': 0.36046027697622773, 'num_tokens': 1384826.0, 'mean_token_accuracy': 0.9511011680960655, 'epoch': 0.41}
{'loss': 0.2791, 'grad_norm': 2.0380470752716064, 'learning_rate': 8.10126582278481e-05, 'entropy': 0.63437658816576, 'num_tokens': 2077226.0, 'mean_token_accuracy': 0.9536829972267151, 'epoch': 0.61}
{'loss': 0.273, 'grad_norm': 1.0367865562438965, 'learning_rate': 3.881856540084388e-05, 'entropy': 0.7885683465003968, 'num_tokens': 2769638.0, 'mean_token_accuracy': 0.954097889661789, 'epoch': 0.82}
{'train_runtime': 4317.1704, 'train_samples_per_second': 2.724, 'train_steps_per_second': 0.057, 'train_loss': 1.3450670514787946, 'entropy': 0.910998734500673, 'num_tokens': 3392760.0, 'mean_token_accuracy': 0.9548680745893054, 'epoch': 1.0}

============================================================
ðŸ’¾ SAVING MODEL
============================================================
âœ“ Done. Output in: /hpcstor6/scratch01/e/edward.gaibor001/medgemma_runs/omama256_balanced

ðŸ’¡ Next step:
  Update evaluation script to use: /hpcstor6/scratch01/e/edward.gaibor001/medgemma_runs/omama256_balanced
============================================================


==============================================
âœ… Step 3 Complete
End time: Fri Oct 17 08:42:02 PM EDT 2025
==============================================

ðŸ“‹ Next steps:
  sbatch run/4-evaluate-balanced.slurm
  sbatch run/5-baseline-comparison.slurm
==============================================
