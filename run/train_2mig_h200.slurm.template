#!/bin/bash
#SBATCH --job-name=omama-1xH200
#SBATCH -A cs_daniel.haehn
#SBATCH -p pomplun
#SBATCH --ntasks=1
#SBATCH --gres=gpu:h200
#SBATCH --mem=200G
#SBATCH -w chimera21
#SBATCH -t 2-24:00:00
#SBATCH --output=/hpcstor6/scratch01/e/edward.gaibor001/logs/%x-%j.out
#SBATCH --error=/hpcstor6/scratch01/e/edward.gaibor001/logs/%x-%j.err


# --- keep caches off HOME ---
export HF_HOME=/hpcstor6/scratch01/e/edward.gaibor001/.hf_cache
export HUGGINGFACE_HUB_CACHE=$HF_HOME
export HF_DATASETS_CACHE=$HF_HOME/datasets
export TRANSFORMERS_CACHE=$HF_HOME/transformers

# (if gated model) - Replace with your actual token
export HUGGINGFACE_HUB_TOKEN=YOUR_HF_TOKEN_HERE

# Memory-friendly knobs for ~33GB per MIG slice
export BS=2                 # per-device batch
export GA=4                 # gradient accumulation (global batch = BS*GA*GPUs)
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
# Uncomment if you need to shrink memory further:
# export USE_4BIT=1

# set any new env knobs for this run
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1

# Activate miniconda
source /home/edward.gaibor001/miniconda3/etc/profile.d/conda.sh
conda activate medgemma

nvidia-smi

cd /home/edward.gaibor001/medgemmaOMAMA

echo "==== Starting training at $(date) ===="
# srun --gres=gpu:h200:1 python 4-modelH.py
srun --gres=gpu:h200:2 python 4-modelH.py
echo "==== Training finished at $(date) ===="
